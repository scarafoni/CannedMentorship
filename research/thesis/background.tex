John McCarthy was the first to formally identify the concept of commonsense reasoning in literature, and described it as 

\begin{center}
 ``the ability of an agent to deduce a sufficiently wide class of immediate consequences of anything if it is told what it already knows'' \cite{mccarthy1963programs}.
\end{center}
  
For his seminal Comirit project, Dr. Benjamin Johnston completely rejected any definition of common sense period \cite{johnston2008comirit}. 
Over the course of all surveyed literature, there are a number of consistencies in the various definitions (both implicit and explicit) of commonsense reasoning in artificial intelligence. 
Some common themes are:
\begin{center}
\begin{enumerate}
	\item The ability to quickly reason on a series of mundane topics and issues
	
	\item Deduction capabilities on high-level, well known topics to which the system has been exposed previously
	
	\item knowledge of a wide variety of topics and of relationships and interactions of entities within
	
	\item an emphasis on rapidity of reasoning rather than accuracy or precision
\end{enumerate}
\end{center}

The use of symbolic logic is also worth noting in the field of commonsense reasoning. Symbolic reasoning has been an integral part of a number of programs meant to emulate common sense. Dr Johnston's Comirit utilizes a tableaux reasoning which uses proof by resolution in conjunctive normal form propositional logic in order to make logical deductions \cite{johnston2008comirit,johnston2009practical}. the Soar project also utilizes a symbolic logic processing unit in addition to it 3D simulations in a very similar manner \cite{wintermute2009overview}. McCarthy's original advice-taker and advice-giver programs were entirely symbolic, relying on forward inference in order to make deductions and reach conclusions based on the given data in its knowledge base and input from the user \cite{mccarthy1963programs}.
Although statistical methods could potentially be used in this field of AI, the vast majority of literature utilizes symbolic or logical methods.

\section{Commonsense Problem Formulation}
Commonsense reasoning problems are approached from a different perspective from that of other AI problems. While many AI agents attempt to create an agent that can efficiently execute one task that supposedly requires ``intelligence,'' commonsense reasoning problems are broader, and less deal with a less precise form of knowledge. For this reason, commonsense problems are often solved with general solutions with less precise metrics than other problems \cite{johnston2008comirit,johnston2009practical}.

One of the first commonsense problems was the ``Tweety problem.'' This involved a human feeding a story into an AI agent; this story progressively described an entity ``Tweety.'' It was proposed that an agent with common sense would be able to infer that Tweety was a bird, and thus infer that it could likely fly. The agent would also be able to adjust it's mental preception of the bird, perhaps inferring it to be a flightless bird, such as a penguin, if it was later stated that Tweety could not fly \cite{johnston2008comirit,egg-cracking}. This problem requires a broad thinking over the general properties of a number of topics and objects, and the inference process is inexact and nebulous.

The ``egg-cracking'' problem is also one of the more famous lemmas of commonsense reasoning. In this problem, as with the Tweety problem, an agent reads a story about a chef making a cake (one of the steps of which is cracking an egg on the bowl and pouring the contents inside). The agent is then asked a series of questions about the scenario. These interrogations usually have a broad range, but would be considered trivial to a human (``why doesn't the chef throw the whole egg into the bowl?'' ``What will happen if the chef does all the motions very slowly?'') \cite{egg-cracking}. In sum, the primary features of commonsense benchmark problems can be summarized in the following points:
\begin{center}
	\begin{itemize}
		\item open problem domains
		\item evaluation based on the answering of na\"{i}ve questions
		\item focus on ``breadth'' knowledge, rather than ``depth''
	\end{itemize}
\end{center}

\section{Significance}
The importance of a system to reason in common sense is readily apparent to the non-technical thinker. 
Nearly every task a human undertakes requires an implicit knowledge of everyday commonsense, and as such, the field of reasoning is almost inseparable from most of the end goals intelligent artificial agents \cite{johnston2009practical, johnston2008comirit}. 
Tasks such as navigating a road during rush hour and pouring wine into a glass properly require naive knowledge of a variety of topics, such as traffic etiquette, physical properties of liquids and solids, acceleration and deceleration of automobiles, etc. There is also a suprising robust amount of existing data for commonsense systems. Johnston cites that many video games implement commonsense reasoning in the form of enemy AI \cite{johnston2008comirit}. Further, common sense is, by definition, possessed by a large percentage of the human population. Because of this, there is a very large potential workforce for compiling or otherwise working with common sense reasoning systems. This has already been implemented with an online application for creating common 3D figures \cite{johnston2011collection}.

The generality of commonsense reasoning is one of its primary benifits. Almost any real world problem will have to deal with an open domain. It is clear that the world does not operate on anything even approximating the extremely controlled and limited worlds commonly evaluated in most computer environments. If an AI agent is to solve real-world problems, it must be able to solve more than a very narrow range of problems, and deal with the numerous contingencies that arise in real world scenarios. \cite{egg-cracking,johnston2009practical,johnston2008comirit,schubert2000episodic,mccarthy1963programs}.

\section{3D Simulations}
Hand in hand with commonsense reasoning is the role of 3D simulations. 
In an informal sense, a simulation is the ability to construct (or in some cases, reconstruct) physical scenes from symbolic data. 
More formally, a simulation is a type of analogous representation which begins with a description of a system which changes based on a causal description of the system \cite{gardin1989analogical}. Simulations have been used many times in the course of both AI and cognitive science. McCarthy first suggested using simulation in his ``advice taker'' program. Since then, the concept has reappeared many times in literature. In particular, physical systems (those representing real-life physics) have implemented with simulation.

 Previous examples of this in Gardin et al's use of “atomic spheres” to model liquids and their properties. This example implements spherical objects (analogous to atoms) which behave collectively as a liquid. The simulation of gravity along with these structures enabled a primitive AI to learn to correctly fill a glass. Johnston was able to accomplish a the same results with Comirit's \cite{gardin1989analogical,johnston2008comirit}. The Soar project also implemented 3D simulations; by creating a 3D environment and testing object positions within, the system was able to deduce rudimentary physical properties and solve related problems \cite{wintermute2009overview}.

The relation between 3D simulations and commonsense reasoning is apparent. As mentioned earlier, commonsense reasoning's primary domain is real-life situations. Human beings are creatures who live in a environment with three spatial dimensions and one time dimension, physical laws are well defined and inescapable in the real world. This means that the most apparent and relevant environment for solving commonsense lemmas is one which incorporates the properties and aspects of the real world \cite{johnston2008comirit,johnston2009practical}.

Cognitive science has observed that humans process their physical environments as a series of simulations: forming an initial mental image and adjusting it over time in order to deduce properties about it \cite{gardin1989analogical}. 
Thus, there is considerable evidence and motivation for utilizing simulations in AI, particularly in a program designed to model and reason in physical situations.
The implementation of such a system as a specialist in the Epilog system also meshes well with previous experiments and evidence.

The incredibly complex spatial situations of three-dimensional simulations are far to computationally intense and convoluted for a symbolic logic system to represent by itself. 
The aid of a specialist provides a more narrow computational domain which can soundly and quickly compute responses to queries over a sub-domain of problems (in our case, problems pertaining to reasoning in spatial environments).

\section{Specialist Programs}
The usage of specialist programs (as opposed to stand alone programs) has shown to be an effective means of programming common sense. While many programs exist as single-component entities, a large proportion of AI programs, as mentioned earlier, deal with a much narrower scope than commonsense reasoning problems \cite{johnston2008comirit,johnston2009practical}. Because of this, there is less of a need for specialist programs because the entire program is a specialist. However, commonsense reasoning requires breadth, not depth, and thus encounters the problems predicted by the clich\`{e}, though true, axiom of software engineering, and life in general: one cannot be good at everything.

Resolution theory provides a formal justification for specialist programs. The central premise of this is that if a given set of clauses each contains a set of sub-clauses which are mutually incompatible (contradictory), then at least one of those clauses need not be true. This allows a considerable narrowing down of the state space needed in order to solve a problem. Because of this, mutual incompatibility of logical atoms are essential to specialist construction. Schubert et al successfully implemented specialists for Epilog for time, color, and part relationships \cite{schubert1983determining,schubert1987accelerating,schubert2000episodic}. 

Thus, commonsense programs will likely need to be broken down into functional units. One of these units would specialize in 3D spatial reasoning. As mentioned above, it is infeasible and awkward to compute problems relating to 3D space in a purely symbolic environment. These computations, however, are very necessary for a true commonsense reasoning system. This lays the foundation for the motivation for the \TDS project. 

\section{Usage in the \TDS and Connection to Epilog}
The Epilog Project is a natural language processing program which seeks to implement commonsense natural language understanding. The system is distinct from other, more traditional means of representing natural languages formally (such as first-order logic) in that it allows for complex and rich concepts such as belief systems, and, as the name of the system suggests, explicit references to time frames. It is from this feature that the name Epilog (EPIsodic LOGic) is derived \cite{schubert2000episodic}.

It is apparent as to how this focus on time frames is closely related to simulation. Just as simulation environments involve updating scenes according to a set of defined rules and relationships as the passage of time occurs, Episodic Logic seeks to divide the world into distinct moments in time and create a logical framework from this \cite{selman1998analogical,schubert2000episodic}.

Epilog does not, however, contain methods for accurately or quickly evaluating stories that exist in 3D environments, nor use knowledge based in this frame of reference. Epilog also has a well defined interface for creating and connecting specialist programs to the larger program. This high level of modularity means that specialist programs can be created easily for the greater Epilog project \cite{schubert2000episodic}. The best solutions is, therefore, to utilize the ``time-slice'' centric approach of Episodic logic, as well as the existing specialist interface, and construct a specialist for reasoning in 3D environments.

In keeping with the established paradigm for commonsense program evaluation, it is most appropriate to base the evaluation of the specialist with a test of a broad range of knowledge that operates at a level that is considered ``simplistic'' by human standards. For this reason, childrens' stories provide an excellent source. The following passage, taken from from Lesson 32, Harris et al. 1889, provides information on a visually rich scene, one which requires a wide range of knowledge of spatial relations and laws in order to understand correctly:

\begin{center}
	\begin{enumerate}[topsep=0pt,itemsep=-1em]
		\item Oh, Rosy! Do you see that nest in the apple tree?
		\item Yes, yes, Frank; I do see it.
		\item Has the nest eggs in it, Frank?
		\item I think it has, Rosy.
		\item I will get into the tree.
		\item Then I can peep into the nest.
		\item Here I am, in the tree.
		\item Now I can see the eggs in the nest.
		\item Shall I get the nest for you, Rosy?
		\item No, no, Frank! Do not get the nest.
		\item Do not get it, I beg you.
		\item Please let me get into the tree, too.
		\item Well, Rosy, here is my hand.
		\item Now! Up, up you go, into the tree.
		\item Peep into the nest and see the eggs.
		\item Oh, Frank! I see them!
		\item The pretty, pretty little eggs!
		\item Now, Frank, let us go.
	\end{enumerate}
\end{center}

This story covers a number of physical scenarios and implicit information that can only be deduced with a sufficient understanding of the spatial laws of nature. For example, a specialist should be able to simulate a tree, two children (Rosie and Frank) the nest in the tree, and the fact that the eggs in the nest are visible from above the nest, but not below. This presents a broad knowledge scenario which tests the abilities of an agent to reason in an open-ended, commonsense environment. This is consistent with the evaluation methods for previous entries in literature, and as such is a good metric for the \TDS \cite{egg-cracking,johnston2008comirit,schubert2000episodic}. For the demonstrative purposes of this project, the 3D Specialist should be able to do the following:

\begin{center}
	\begin{enumerate}
		\item construct a scene where
		\begin{itemize}
			\item a person is under a tree
			\item a nest is in a tree
			\item an egg is in the nest
			\item the person can see the nest
		\end{itemize}
		\item deduce that the person can see the nest (though the vision may be obscured by branches)
		\item deduce that the person cannot see the egg (because the nest is in the way)
	\end{enumerate}
\end{center}

This scenario provides a diverse assortment of predications, objects, and conjectures about the story, such that the above tasks will be an important indication of whether or not the system is capable of larger, more complex inferences in commonsense reasoning.

\section{Object Construction and Placement}
From the more concrete aspect of 3D simulation, the methods by which objects are modeled in the 3D scene, and the methods by which they are placed to satisfy logical constraints, are a matter of considerable importance.

Most of the simulation work surveyed constructs objects in a very similar fashion. 3D object made of polygons bound together are the norm for 3D simulation \cite{xu2002constraint,wintermute2009overview,wordseye}. Further, there is a plethora of existing software that utilizes this ``face, mesh, vertex'' paradigm. Thus it makes the most sense to utilize this model in order to capitalize on the fruits of previous research.

Both Soar, Wordseye, and CAPS utilized similar placement methods for objects. Soar utilized the concept of ``legal areas:'' distinct areas of three-dimensional space that represented potential locations for object placement that satisfied predications \cite{wintermute2009overview}. Similarly, the CAPS project implemented ``legal polygons,'' the two dimensional equivalent \cite{xu2002constraint}. Wordseye uses a combination of the two: both three-dimensional placement areas and two-dimensional legal polygons, as a means of specifying areas where object can be placed in order to satisfy predications. In addition, Wordseye also object ``tagging,'' labeling certain parts of objects as relevant areas for certain predications, as a means to aid in object placement. For example, the hand of a person model is tagged with as being able to hold certain objects. If the scene requires the person to be holding an apple, the area specified by this tag becomes the critical point.